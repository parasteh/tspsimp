{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboard_logger import Logger as TbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nets.critic_network import CriticNetwork\n",
    "from train import train_epoch\n",
    "from nets.reinforce_baselines import CriticBaseline\n",
    "from nets.attention_model import AttentionModel\n",
    "from utils import torch_load_cpu, load_problem, get_inner_model, move_to\n",
    "from utils.plots import plot_tour\n",
    "from siam_ensemble import validate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['validate-83.pt',\n",
       " 'validate-95.pt',\n",
       " 'validate-93.pt',\n",
       " 'validate-74.pt',\n",
       " 'validate-86.pt',\n",
       " 'validate-88.pt',\n",
       " 'validate_swa-90.pt',\n",
       " 'validate_swa-80.pt',\n",
       " 'validate-77.pt',\n",
       " 'validate-90.pt']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from os import listdir \n",
    "from os.path import isfile,join \n",
    "from random import sample \n",
    "\n",
    "# load the run args\n",
    "%run options\n",
    "mypath = './outputs/tsp_20/run_name_20220326T164210/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "condidate_models = []\n",
    "for name in onlyfiles:\n",
    "    \n",
    "    if name != 'args.json' and name[0] == 'v' and int(name.split('-')[1].split('.')[0])> 70:\n",
    "        condidate_models.append(name)\n",
    "\n",
    "selected_models = sample(condidate_models,10)\n",
    "selected_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "torch.manual_seed(opts.seed)\n",
    "\n",
    "# Optionally configure tensorboard\n",
    "tb_logger = None\n",
    "if not opts.no_tensorboard:\n",
    "    tb_logger = TbLogger(os.path.join(opts.log_dir, \"{}_{}\".format(opts.problem, opts.graph_size), opts.run_name))\n",
    "\n",
    "if not os.path.exists(opts.save_dir):\n",
    "    os.makedirs(opts.save_dir)\n",
    "\n",
    "# Save arguments so exact configuration can always be found\n",
    "with open(os.path.join(opts.save_dir, \"args.json\"), 'w') as f:\n",
    "    json.dump(vars(opts), f, indent=True)\n",
    "\n",
    " # Set the device\n",
    "opts.device = torch.device(\"cuda\" if opts.use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP with 20 nodes.\n"
     ]
    }
   ],
   "source": [
    "problem = load_problem(opts.problem)(\n",
    "                        p_size = opts.graph_size, # tsp size\n",
    "                        with_assert = not opts.no_assert) # if checking feasibiliy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionModel(\n",
       "  (embedder): EmbeddingNet(\n",
       "    (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): MultiHeadAttentionLayer(\n",
       "      (0): SkipConnection(\n",
       "        (module): MultiHeadAttention()\n",
       "      )\n",
       "      (1): Normalization(\n",
       "        (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): SkipConnection(\n",
       "        (module): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.001, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (4): Dropout(p=0.001, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Normalization(\n",
       "        (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): MultiHeadAttentionLayer(\n",
       "      (0): SkipConnection(\n",
       "        (module): MultiHeadAttention()\n",
       "      )\n",
       "      (1): Normalization(\n",
       "        (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): SkipConnection(\n",
       "        (module): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.001, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (4): Dropout(p=0.001, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Normalization(\n",
       "        (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): MultiHeadAttentionLayer(\n",
       "      (0): SkipConnection(\n",
       "        (module): MultiHeadAttention()\n",
       "      )\n",
       "      (1): Normalization(\n",
       "        (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): SkipConnection(\n",
       "        (module): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.001, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (4): Dropout(p=0.001, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Normalization(\n",
       "        (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (decoder): MultiHeadDecoder()\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = {\n",
    "    'attention': AttentionModel,\n",
    "}.get(opts.model, None)\n",
    "assert model_class is not None, \"Unknown model: {}\".format(model_class)\n",
    "                                                           \n",
    "model = model_class(\n",
    "    problem = problem,\n",
    "    embedding_dim = opts.embedding_dim,\n",
    "    hidden_dim = opts.hidden_dim,\n",
    "    n_heads = 1, # can increase for better performance\n",
    "    n_layers = opts.n_encode_layers,\n",
    "    normalization = opts.normalization,\n",
    "    device = opts.device,\n",
    ").to(opts.device)\n",
    "\n",
    "if opts.use_cuda and torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " ),\n",
       " AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " ),\n",
       " AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " ),\n",
       " AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " ),\n",
       " AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " ),\n",
       " AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " ),\n",
       " AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " ),\n",
       " AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " ),\n",
       " AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " ),\n",
       " AttentionModel(\n",
       "   (embedder): EmbeddingNet(\n",
       "     (embedder): Linear(in_features=2, out_features=128, bias=True)\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (2): MultiHeadAttentionLayer(\n",
       "       (0): SkipConnection(\n",
       "         (module): MultiHeadAttention()\n",
       "       )\n",
       "       (1): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "       (2): SkipConnection(\n",
       "         (module): Sequential(\n",
       "           (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (1): ReLU()\n",
       "           (2): Dropout(p=0.001, inplace=False)\n",
       "           (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (4): Dropout(p=0.001, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): Normalization(\n",
       "         (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (project_graph): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (project_node): Linear(in_features=128, out_features=128, bias=False)\n",
       "   (decoder): MultiHeadDecoder()\n",
       " )]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(model_path):\n",
    "        # Load data from load_path\n",
    "    load_data = {}\n",
    "    assert opts.load_path is None or opts.resume is None, \"Only one of load path and resume can be given\"\n",
    "    load_path = opts.load_path if opts.load_path is not None else opts.resume\n",
    "    if load_path is not None:\n",
    "        print('  [*] Loading data from {}'.format(load_path))\n",
    "        load_data = torch_load_cpu(load_path)\n",
    "\n",
    "    # Overwrite model parameters by parameters to load\n",
    "    model_ = get_inner_model(model)\n",
    "    model_.load_state_dict({**model_.state_dict(), **load_data.get('model', {})})\n",
    "    return model\n",
    "models = []\n",
    "for path in selected_models:\n",
    "    models.append(load_model(mypath+path))\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3333333333333335"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged: (num_averaged * averaged_model_parameter + model_parameter)/ (num_averaged + 1)\n",
    "ema_avg(4,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3333333333333335"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged):\n",
    "    return (num_averaged * averaged_model_parameter + model_parameter)/ (num_averaged + 1) \n",
    "avg_fn(4,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4509, 0.8512],\n",
       "        [0.3281, 0.4170],\n",
       "        [0.2536, 0.4560],\n",
       "        [0.6566, 0.6865],\n",
       "        [0.0264, 0.5931],\n",
       "        [0.2647, 0.8877],\n",
       "        [0.9084, 0.8208],\n",
       "        [0.3501, 0.1872],\n",
       "        [0.9299, 0.0127],\n",
       "        [0.1770, 0.8813],\n",
       "        [0.1611, 0.9836],\n",
       "        [0.6462, 0.2009],\n",
       "        [0.5012, 0.6662],\n",
       "        [0.4446, 0.8552],\n",
       "        [0.5352, 0.5726],\n",
       "        [0.9432, 0.0668],\n",
       "        [0.6561, 0.2709],\n",
       "        [0.7097, 0.8725],\n",
       "        [0.6250, 0.1875],\n",
       "        [0.6853, 0.0979]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('datasets/tsp_20_10000.pkl','rb') as input_file:\n",
    "    test_set = pickle.load(input_file)\n",
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rollout:   0%|                    | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (400) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_151781/3317851647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/TSP-improve-AI/siam_ensemble.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(problem, models, val_dataset, tb_logger, opts, _id, is_swa)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# run the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0ms_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         bv, improve, r, _ = rollout(problem,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                     \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                     \u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/TSP-improve-AI/siam_ensemble.py\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(problem, models, x_input, batch, solution, value, opts, T, do_sample, record)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm_values\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msolutions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexchange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0msum_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (400) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "validate(problem, models, test_set[:100], tb_logger, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(20,20)\n",
    "y = torch.zeros(20,20)\n",
    "\n",
    "x.add(y) /2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
